# [Efficiency and Order of growth]()

Efficiency is about algorithms not about coding details

The clue is to reduce the problem to a previously solved problem

**2 dimensions: space & time**

### computational complexity
Is not a stable mesure:
- speed of machine
- cleverness of python implementation
- depends upon input
count the number of basic steps
```
T(Time) : N(Natural num) --> N
          ^                  ^
       size of           number of
        input              steps
```
a **step** is an operation that takes constant time

### Random Access Machine (RAM)
- sequiential (instructions executed one after another)
- memory access in constant time

**Best case:** minimum running time over all possible inputs
**Worst case:** maximum running time over all possible inputs (complexity analysis almost always focus on worst case)
- upper bound: how bad things can happen
- happens often
**Expected case:** average (barely used in algorithm analysis)

If steps number follows:
`2 + 3n + 1`

**Growth with respect to size of input**. Addittive constants have to be ignored resulting:
`3n`

Typically, we even ignore multiplicative constants:
`n`
Assymptotic growth. How the complexity grows as you reach the limit as you reach the limit of the size of the input.

Big Oh notation: `O(n)` the commplexity of the algorithm grows linearly with `n`
It gives an upper bound: `F(x) âˆˆ O(x^2)` Function F grows no faster than the quadratic polynomial `x^2`

`O(1)` : constant
`O(log n)` : logarithmic growth
`O(n)` : linear
`O(n log n)` : log linear
`O(n)` : linear
`O(n^c)` : polynomial
`O(c^n)` : exponential

